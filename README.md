# Local_RAG_Agent
# ğŸ§  Local RAG Agent (LCEL)

A **local AI question-answering agent** built using **LangChain Expression Language (LCEL)**.
It can answer user questions based on the contents of a provided file â€” entirely **offline**, using local models and vector search.

---

## ğŸš€ Features

* **Retrieval-Augmented Generation (RAG):** Answers are grounded in your documentâ€™s content.
* **Local Execution:** Uses `Ollama` + `FAISS` + `HuggingFace` embeddings â€” no external API calls.
* **Composable LCEL Pipeline:** Fully modular and extensible with LangChain Expression Language.
* **Supports Any Text File:** Plug in `.txt`, `.md`, or even processed PDF text.

---

## ğŸ§© Architecture Overview

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Document  â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Text Splitter     â”‚ â†’ chunks for embeddings
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Embeddings Model  â”‚ (MiniLM)
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Vectorstore (FAISSâ”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ LCEL Chain: Retriever â†’ LLM     â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Setup Instructions

### 1ï¸âƒ£ Clone & Install

```bash
git clone https://github.com/yourusername/local-rag-agent.git
cd local-rag-agent

pip install -r requirements.txt
```

### 2ï¸âƒ£ Dependencies

Ensure the following are installed:

```bash
pip install langchain langchain-community langchain-text-splitters faiss-cpu sentence-transformers langchain-ollama
```

### 3ï¸âƒ£ Install Ollama and Pull a Model

```bash
brew install ollama
ollama pull mistral
```

You can also use `llama3`, `phi3`, or any other supported model.

---

## ğŸ“˜ Usage

1. Place your reference text file (e.g., `data.txt`) in the root directory.
2. Run the agent:

```bash
python local_rag_agent.py
```

3. Start chatting:

```
ğŸ§  Local RAG Agent (LCEL) Ready!
Ask a question: What is this document about?
```

Type `exit` to quit.

---

## ğŸ§± Project Structure

```
local-rag-agent/
â”‚
â”œâ”€â”€ data.txt                  # Your knowledge source
â”œâ”€â”€ local_rag_agent.py        # Main RAG agent script
â”œâ”€â”€ requirements.txt          # Dependencies
â””â”€â”€ README.md                 # Project documentation
```

---

## âš™ï¸ How It Works

1. **Load Document:** Text is read and chunked into overlapping segments.
2. **Embed & Index:** Each chunk is embedded using `sentence-transformers` and stored in a FAISS index.
3. **Retrieve:** When you ask a question, top-k similar chunks are fetched.
4. **Generate:** The retrieved chunks + question form a context-aware prompt, passed to the local LLM.
5. **Answer:** The model outputs a grounded, contextually accurate answer.

---

## ğŸ§® LCEL Chain Definition

```python
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
```

Each `|` represents a flow stage â€” retrieval â†’ prompt â†’ generation â†’ parsing.
This is the core of LangChain Expression Language.

---

## ğŸ§  Customization

* **Change Model:** Replace `mistral` with `llama3`, `phi3`, etc.
* **Adjust Chunk Size:** Tune `chunk_size` and `chunk_overlap` for optimal context.
* **Add Memory:** Use `ConversationBufferMemory` for contextual conversations.
* **Add Persistence:** Swap FAISS with ChromaDB for saved indexes.

---

## ğŸ“ˆ Future Improvements

* PDF & Markdown ingestion
* Web UI (Streamlit or Gradio)
* LangGraph integration for multi-tool reasoning
* Persistent vectorstore cache

---

> â€œRAG turns language models into knowledge models â€” grounded, truthful, and local.â€
