# ğŸ§  Local RAG Agent (ChromaDB + LangChain)

A minimal **Retrieval-Augmented Generation (RAG)** system built with
**LangChain Expression Language (LCEL)**, **ChromaDB**, and **Ollama** for fully local operation.
It answers questions from `.txt` or `.csv` files by retrieving relevant context and generating responses via a local LLM.

---

## ğŸš€ Features

* ğŸ“„ Supports both `.txt` and `.csv` files
* ğŸ§  Uses **ChromaDB** as a local vector database
* ğŸ”¤ Generates **embeddings** with `OllamaEmbeddings`
* ğŸ¤– Answers questions using a local Ollama model (e.g. `gpt-oss:120b-cloud`, `mistral`, `llama3`, etc.)
* âš™ï¸ Built with **LangChain Expression Language (LCEL)** for composable pipelines

---

## ğŸ§± Project Structure

```
local-rag-chroma/
â”‚
â”œâ”€â”€ data.txt                # or data.csv
â”œâ”€â”€ local_rag_agent.py      # main RAG agent
â”œâ”€â”€ vectorstore_utils.py    # handles text loading + vectorstore
â”œâ”€â”€ requirements.txt        # dependencies
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup

### 1ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Install Ollama and a model

```bash
brew install ollama
ollama pull gpt-oss:120b-cloud
```

---

## ğŸ§  Usage

1. Place your `.txt` or `.csv` file in the project folder.
2. Run the agent:

   ```bash
   python local_rag_agent.py
   ```
3. Ask questions about your data:

   ```
   Ask a question: What is LangChain?
   Answer: LangChain is a framework for building applications with large language models.
   ```

---

## ğŸ§© How It Works

1. Loads your file and splits text into chunks.
2. Embeds chunks using OllamaEmbeddings embeddings.
3. Stores embeddings locally in ChromaDB.
4. Retrieves top matches for your question.
5. Generates an answer with Ollama LLM.

---

